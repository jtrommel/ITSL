\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{An Introduction to Statisical Learning - own work}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=ITSL}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(broom)
library(ISLR)
library(gridExtra)
library(plot3D)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions

\frontmatter
\chapter*{Introduction to Statistical Learning - own work}

\mainmatter
\chapter{Introduction}

\section{Overview of Statistical Learning}

\subsection{A regression problem: Wage Data}
Continuous reponse variable (wage) with three independent variables: age (continous), year (discrete integer), education level (factor). A \emph{regression} problem. The graphs show a non-linear relationship with ''age", a linear relation with ''year"  and a dependence on the factor ''education level". But all variables show a lot of variation.
<<label=1Wage,fig=TRUE,include=FALSE, echo=FALSE>>=
p1 <- ggplot(data=Wage, aes(x = age, y = wage)) +
  geom_point(alpha=0.1) +
  geom_smooth() +
  labs(x="Age", y="Wage") +
  JT.theme
p2 <- ggplot(data=Wage, aes(x = year, y = wage)) +
  geom_point(alpha=0.1) +
  geom_smooth(method=lm) +
  labs(x="Year", y="Wage") +
  JT.theme
p3 <- ggplot(data=Wage, aes(x = education, y = wage)) +
  geom_boxplot(aes(colour=education)) +
  labs( x="Education level", y="Wage") +
  scale_x_discrete(labels = c('1','2','3', '4', '5')) +
  theme(legend.position="none") +
  JT.theme
grid.arrange(p1, p2, p3, nrow=1)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{ITSL-1Wage}
\caption{Relations between Wage and main predictors}
\label{fig:1Wage}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

\subsection{A classification problem: Stock Market Data}
A categorical reponse variable (increase of decrease of the index) with independent variable: the x days' percentage changes of the index (x=1, 2, 3). A \emph{classification} problem. 

<<label=1Smarket,fig=TRUE,include=FALSE, echo=FALSE>>=
p1 <- ggplot(data=Smarket) +
  geom_boxplot(aes(x=Direction, y= Lag1, colour=Direction)) +
   theme(legend.position="none") +
  labs(title="Yesterday", x="Today's Direction", y="Percentage change in S&P") +
  JT.theme
p2 <- ggplot(data=Smarket) +
  geom_boxplot(aes(x=Direction, y= Lag2, colour=Direction)) +
   theme(legend.position="none") +
  labs(title="Two Days Previous", x="Today's Direction", y="Percentage change in S&P") +
  JT.theme
p3 <- ggplot(data=Smarket) +
  geom_boxplot(aes(x=Direction, y= Lag3, colour=Direction)) +
   theme(legend.position="none") +
  labs(title="Three Days Previous", x="Today's Direction", y="Percentage change in S&P") +
  JT.theme
grid.arrange(p1, p2, p3, nrow=1)
@

\begin{figure}
\includegraphics[width=1\textwidth]{ITSL-1Smarket}
\caption{Classification based on previous change}
\label{fig:1Smarket}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

\subsection{A clustering problem: Gene Expression Data (NCI60)}
Only input variables. We are not trying to predict an outcome but are looking for similarities. 

<<label=1NCI60,fig=TRUE,include=FALSE, echo=FALSE>>=
nci.labs <- NCI60$labs
nci.data <- NCI60$data
table(nci.labs)
pr.out <- prcomp(nci.data, scale=TRUE)
Cols = function(vec) {
  cols <- rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19, xlab="Z1", ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19, xlab="Z1", ylab="Z3")
par(mfrow=c(1,1))
@

\begin{figure}
\includegraphics[width=1\textwidth]{ITSL-1NCI60}
\caption{Clustering based on principal component analysis}
\label{fig:1NCI60}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

\chapter{Statistical Learning}

\section{What is statistical learning}

In general: A \emph{response}, \emph{dependent}, \emph{output} variable $Y$ is influenced by a number of \emph{predictor}, \emph{independent}, \emph{input} variables $X_{i}$. We \emph{assume} that the relationschip between $Y$ and the $X_{i}$ can be written as:
\begin{equation}
Y=f(X_{i}) + \epsilon
\end{equation}

where $f(X_{i})$ is an, unknown, but fixed function and $\epsilon$ is a random error term. The function $f(X_{i})$ represent the \emph{systematic information} that the variables $X_{i}$ give about $Y$.

<<echo=FALSE>>=
# Loading the Advertising data
Advertising <- read.csv("data/Advertising.csv", header=TRUE, sep=",")
@

<<label=2Advertising,fig=TRUE,include=FALSE, echo=FALSE>>=
# Making the graphs
ggplot(data=Advertising, aes(x=TV, y= sales)) +
  geom_point(shape=21, color="Red") +
  geom_smooth(method=lm, se=FALSE) +
  JT.theme -> p1
ggplot(data=Advertising, aes(x=radio, y= sales)) +
  geom_point(shape=21, color="Red") +
  geom_smooth(method=lm, se=FALSE) +
  JT.theme -> p2
ggplot(data=Advertising, aes(x=newspaper, y= sales)) +
  geom_point(shape=21, color="Red") +
  geom_smooth(method=lm, se=FALSE) +
  JT.theme -> p3
grid.arrange(p1, p2, p3, nrow=1)
@

\begin{marginfigure}[-5cm]
\includegraphics[width=1\textwidth]{ITSL-2Advertising}
\caption{The \textcolor{red}{Advertising} data set. The plot displays \textbf{sales} as a function of the advertising budget (in thousands of dollars) for adds on TV, radio and in newspapers}
\label{fig:2Advertising}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

In the \textcolor{red}{\textbf{Advertising}} data set, sales ($Y$) are influenced by the amount we spend on advertising in TV ($X_{1}$), on the radio ($X_{2}$) and in the newspapers ($X_{3}$). Figure~\ref{fig:2Advertising} shows simple least squares fits of the dependent variable $Y$ to the variables $X_{1}$, $X_{2}$ and $X_{3}$.

<<echo=FALSE>>=
# Loading Income1 data set
Income1 <- read.csv("data/Income1.csv", header=TRUE, sep=",")
# Guessing a 3th degree polynomial with (from the graph) a value of about 20 for ''years of education"=10, a value of 80 for ''years of education=22, an first derivatives zero at start and end
Income1$f <- 214.44 -45.83*Income1$Education +3.333*(Income1$Education)^2 -0.0694*(Income1$Education)^3
@

The functional relationship is not necessary linear. Figure~\ref{fig:2Income1} gives an example where \textit{Income} is related to \textit{Years of education}.

<<label=2Income1,fig=TRUE,include=FALSE, echo=FALSE>>=
# Making the graphs
ggplot(data=Income1, aes(x=Education, y= Income)) +
  geom_point(shape=21, color="Red") +
  JT.theme -> p1
ggplot(data=Income1, aes(x=Education)) +
  geom_point(aes(y = Income), shape=21, color="Red") +
  geom_line(aes(y =f), color="Black") +
  geom_segment(aes(x=Education, y=Income, xend=Education, yend=f)) +
  JT.theme -> p2
grid.arrange(p1, p2, nrow=1)
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{ITSL-2Income1}
\caption{The \textcolor{red}{Income} data set. The plot displays \textbf{Income} as a function of the number of years of education. The right hand side gives a model and the error for each observation}
\label{fig:2Income1}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

When we include a second independent variable $X_{2}=Seniority$ we can still give a graphical representation in a threedimensional plot (Figure~\ref{fig:2Income2}).

<<echo=FALSE>>=
# Loading Income2 data set
Income2 <- read.csv("data/Income2.csv", header=TRUE, sep=",")
x <- Income2$Education
y <- Income2$Seniority
z <- Income2$Income
@

<<label=2Income2,fig=TRUE,include=FALSE, echo=FALSE>>=
xyz.fit <- lm(z ~ x + y)
x.pred <- seq(min(x), max(x), length.out = 50)
y.pred <- seq(min(y), max(y), length.out = 50)
xy <- expand.grid(x = x.pred, y = y.pred)
z.pred <- matrix(predict(xyz.fit, newdata = xy), 
                 nrow = 50, ncol = 50)
fitpoints <- predict(xyz.fit)
scatter3D(x, y, z, 
          bt = "b2", # box type
          pch = 21, cex = 0.7, col="red", # shape, size , color of data points
          theta = 25, phi = 20, # viewing angel horiz and vert
          ticktype = "detailed", # detailed ticks on axis
          xlab = "Education", # label x-axis
          ylab = "Seniority", # label y-axis
          zlab = "Income",  # label z-axis
          surf = list(x = x.pred, y = y.pred, z = z.pred,  # coordinates of surface
                      facets = FALSE, # coloring of the surface facets
                      fit=fitpoints, # drax line from surface to data point
                      col=gg.col(1,0.5)), # color pattern (1 = only one color) and alpha value
          main = "", # main title
          colkey = FALSE) # no legend
@

\begin{figure}
\includegraphics[width=0.7\textwidth]{ITSL-2Income2}
\caption{The \textcolor{red}{Income} data set. The 3D-plot displays \textbf{Income} as a function of the number of years of education and seniority.}
\label{fig:2Income2}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

\newpage
\subsection{Why estimate the function $f$?}

Once the function $f$ is known we can use it to predict values of the dependent variable $Y$ for given values of the independent variables $X_{i}$ (as long as we stay within the range of $X_{i}$-values for which $f$ is derived!).

\begin{equation}
  \hat{Y}=\hat{f}(X_{i})
\end{equation}

Usually we do not know $f$ exactly, but an approximation of $f$ which we call $\hat{f}$. The accuracy of $\hat{Y}$ depends on two quantities:

\begin{enumerate}
  \item the \emph{reducible} error: this is the error that can be minimised when $\hat{f}$ comes closer to $f$
  \item the \emph{irreducible} error: this is the unknown $\epsilon$ due to unmeasured variables or unknown variables
\end{enumerate}

\newthought{For Prediction} purposes we are not really interested in the actual mathematical form of $\hat{f}$, but only in its usefulness to predict the output $Y$. Essentially: $\hat{f}$ is treated like a \emph{black box}.

\newthought{For Inference} we do want to know the mathematical nature of $\hat{f}$ because we want a better understanding of the model:
\begin{itemize}
  \item which predictors are important?
  \item what is the relationship between the response variable and each of the predictors? Does it increase or decrease?
  \item can the relationship be expressed in a linear way or do we need something more complex?
\end{itemize}

Functions that give more accurate predictions can become opaque and the relation between the dependent variable and the predictors cannot be felt intuitively any more.

\subsection{How to estimate $f$?}

We start with \emph{observations}: a set of n data points, which we call the \emph{training data}. Some notation:
\begin{itemize}
  \item $p$ is the number of independent variables, $n$ is the number of observations
  \item $x_{ij}$ is the $i^{th}$ observation of predictor $x_{j}$
  \item $y_{i}$ is the corresponding value of the output
  \item the training data is $\{ (x_{1}, y_{1}), (x_{2}, y_{2}) \ldots (x_{n}, y_{n}) \}$ with $x_{i}=(x_{i1}, x_{i2} \ldots x_{ip})^{T}$
\end{itemize}

The methods to find $\hat{f}$ are either \emph{parametric} or \emph{non-parametric}.

\newthought{Parametric methods} use a two-step approach:
\begin{enumerate}
  \item assume the shape of $f$:
    \begin{equation}
      f(X) = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \ldots + \beta_{p}X_{p}
    \end{equation}
    This choice (a simple linearity in $X_{i})$ reduces the problem of finding $f$ to the problem of finding $\beta_{i}$ for $i=1 \ldots p$.
    \item \emph{fit} or \emph{train} the model = estimate the parameters $\beta_{i}$ for $i=1 \ldots p$. There are different methods: \emph{ordinary least squares} or \emph{maximise the log-likelihood function}
\end{enumerate}

The advantage of the parametric approach is that it is simple to do. The disadvantage is that the result can be markedly different from reality. Adding more and more parametes can improve the model, but it leads to \emph{overfitting} which makes the model follow the \emph{noise} instead of the signal.

Figure~\ref{fig:2Income2par} assumes a linear relation of Income with Education and Seniority. But it does not have the curvature that seems to be in the data. A non-linear function might do better.

<<label=2Income2par,fig=TRUE,include=FALSE, echo=FALSE>>=
scatter3D(x, y, z, 
          bt = "b2", # box type
          pch = 21, cex = 0.7, col="red", # shape, size , color of data points
          theta = 25, phi = 20, # viewing angel horiz and vert
          ticktype = "detailed", # detailed ticks on axis
          xlab = "Education", # label x-axis
          ylab = "Seniority", # label y-axis
          zlab = "Income",  # label z-axis
          surf = list(x = x.pred, y = y.pred, z = z.pred,  # coordinates of surface
                      facets = FALSE, # coloring of the surface facets
                      fit=fitpoints, # drax line from surface to data point
                      col=gg.col(1,0.5)), # color pattern (1 = only one color) and alpha value
          main = "Parametric function: a linear relationship without interaction", # main title
          colkey = FALSE) # no legend
@

\begin{marginfigure}[-7cm]
\includegraphics[width=1\textwidth]{ITSL-2Income2par}
\caption{The \textcolor{red}{Income} data set. The 3D-plot displays \textbf{Income} as a function of the number of years of education and seniority.}
\label{fig:2Income2par}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

\newthought{Non-parametric models} do not make any assumption about the mathematical form of $f$. \emph{splines} are an example of a non-parametric solution. It does not impose a pre-specified model on $f$. On the other hand: it does not supply any symbolic representation of $f$ either. Making the spline smoother and smoother will in the end give us a perfect fit with the observations, but also with the noise ($\epsilon$) that is always present. This is overfitting.

The R package \textbf{rms} has functions that calculate a spline surface through the data points. It also has drawing functions, but it is better to transform the results into the data-format needed by 3D-drawing package \textbf{plot3D}.

<<label=2Income2nonpar,fig=TRUE,include=FALSE, echo=FALSE>>=
library(rms)
# Making the spline model with package rms
att <- Income2[c('Education','Seniority','Income')]
add <- datadist(att)
options(datadist="add")
mdl <- ols( Income ~ rcs(Education,4)*rcs(Seniority,4) ,data=att)
# Using this model we calculate the predicted value for a grid of np point on the x-axis and np-point on the y-axis
np <- 75 # number of points to predict
pred.rms <- Predict(mdl, 'Education','Seniority', np=np)
# Getting the information form the spline-model that the plot3D package needs to make the graph
x.pred.sp <- unique(pred.rms$Education)
y.pred.sp <- unique(pred.rms$Seniority)
z.pred.sp <- matrix(pred.rms$yhat, nrow=np, ncol=np)
# Using the plot3D package to make the 3D drawing
fitpoints <- predict(mdl)
scatter3D(x, y, z,
          bt = "b2", # box type
          pch = 21, cex = 0.7, col="red", # shape, size , color of data points
          theta = 25, phi = 20, # viewing angel horiz and vert
          ticktype = "detailed", # detailed ticks on axis
          xlab = "Education", # label x-axis
          ylab = "Seniority", # label y-axis
          zlab = "Income",  # label z-axis
          surf = list(x = x.pred.sp, y = y.pred.sp, z = z.pred.sp,  # coordinates of surface
                      facets = FALSE, # coloring of the surface facets
                      fit=fitpoints, # drax line from surface to data point
                      col=gg.col(1,0.4)), # color pattern (1 = only one color) and alpha value
          main = "Non-parametric function: a spline through the data points", # main title
          colkey = FALSE) # no legend
@

\begin{marginfigure}[-5cm]
\includegraphics[width=1\textwidth]{ITSL-2Income2nonpar}
\caption{The \textcolor{red}{Income} data set. The 3D-plot displays \textbf{Income} as a spline surfact based on the number of years of education and seniority.}
\label{fig:2Income2nonpar}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

Using the non-parametric methods requires a good understanding of the way splines are calculated. The choice of the spline characteristics can easily lead to under- or overfitting (Figure~\ref{fig:2Income2under} and Figure~\ref{fig:2Income2over}).

<<label=2Income2under,fig=TRUE,include=FALSE, echo=FALSE>>=
mdl <- ols( Income ~ rcs(Education,3)*rcs(Seniority,3) ,data=att)
# Using this model we calculate the predicted value for a grid of np point on the x-axis and np-point on the y-axis
np <- 75 # number of points to predict
pred.rms <- Predict(mdl, 'Education','Seniority', np=np)
# Getting the information form the spline-model that the plot3D package needs to make the graph
x.pred.sp <- unique(pred.rms$Education)
y.pred.sp <- unique(pred.rms$Seniority)
z.pred.sp <- matrix(pred.rms$yhat, nrow=np, ncol=np)
# Using the plot3D package to make the 3D drawing
fitpoints <- predict(mdl)
scatter3D(x, y, z,
          bt = "b2", # box type
          pch = 21, cex = 0.7, col="red", # shape, size , color of data points
          theta = 25, phi = 20, # viewing angel horiz and vert
          ticktype = "detailed", # detailed ticks on axis
          xlab = "Education", # label x-axis
          ylab = "Seniority", # label y-axis
          zlab = "Income",  # label z-axis
          surf = list(x = x.pred.sp, y = y.pred.sp, z = z.pred.sp,  # coordinates of surface
                      facets = FALSE, # coloring of the surface facets
                      fit=fitpoints, # drax line from surface to data point
                      col=gg.col(1,0.4)), # color pattern (1 = only one color) and alpha value
          main = "Non-parametric function: underfitting", # main title
          colkey = FALSE) # no legend
@

<<label=2Income2over,fig=TRUE,include=FALSE, echo=FALSE>>=
mdl <- ols( Income ~ rcs(Education,5)*rcs(Seniority,5) ,data=att)
# Using this model we calculate the predicted value for a grid of np point on the x-axis and np-point on the y-axis
np <- 75 # number of points to predict
pred.rms <- Predict(mdl, 'Education','Seniority', np=np)
# Getting the information form the spline-model that the plot3D package needs to make the graph
x.pred.sp <- unique(pred.rms$Education)
y.pred.sp <- unique(pred.rms$Seniority)
z.pred.sp <- matrix(pred.rms$yhat, nrow=np, ncol=np)
# Using the plot3D package to make the 3D drawing
fitpoints <- predict(mdl)
scatter3D(x, y, z,
          bt = "b2", # box type
          pch = 21, cex = 0.7, col="red", # shape, size , color of data points
          theta = 25, phi = 20, # viewing angel horiz and vert
          ticktype = "detailed", # detailed ticks on axis
          xlab = "Education", # label x-axis
          ylab = "Seniority", # label y-axis
          zlab = "Income",  # label z-axis
          surf = list(x = x.pred.sp, y = y.pred.sp, z = z.pred.sp,  # coordinates of surface
                      facets = FALSE, # coloring of the surface facets
                      fit=fitpoints, # drax line from surface to data point
                      col=gg.col(1,0.4)), # color pattern (1 = only one color) and alpha value
          main = "Non-parametric function: overfitting", # main title
          colkey = FALSE) # no legend
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{ITSL-2Income2under}
\caption{The \textcolor{red}{Income} data set. Effects of underfitting}
\label{fig:2Income2under}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

\begin{marginfigure}
\includegraphics[width=1\textwidth]{ITSL-2Income2over}
\caption{The \textcolor{red}{Income} data set. Effects of overfitting}
\label{fig:2Income2over}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

\subsection{Trade-off between Prediction Accuracy and Model Interpretability}

Some functions are \emph{restrictive} (for example the linear regression models) while others are more \emph{flexible}. There is an inverse relationship between \emph{flexibility} and \emph{interpretability}. \emph{Interpretability} is a measure of the insight the function $f$ gives us into the relationships between the \emph{output variable} and the \emph{predictor variables}. When our goal is \emph{Prediction} than we should opt for \emph{flexibility}. When we choose \emph{Inference} we should go for \emph{restriction}. Figure~\ref{fig:2tradeoff} gives an overview of frequently used methods and their position in the (Flexibility - Interpretability)-plane.

<<label=2tradeoff,fig=TRUE,include=FALSE, echo=FALSE>>=
coords <- data.frame(x=c(2, 3, 4.5, 5.5, 8, 8), y=c(9, 7, 5.5, 5, 2, 1), meth=c("Subset Selection Lasso", "Least Squares", "Generalized Additive Methods", "Trees", "Bagging,Boosting", "Support Vector Machines"), Flexibility=c("Low", "Low", "Middle", "Middle", "High", "High"), Interpretability=c("High", "High", "Middle", "Middle", "Low", "Low"))
ggplot(data=coords, aes(x=x, y=y, label=meth)) +
  geom_point() +
  geom_label(position = position_dodge(0.9)) +
  labs(title="Tradeoff between Flexibility and Interpretability", 
       x="Flexibility", 
       y="Interpretability") +
  scale_x_continuous(breaks=c(2, 5, 8),
                   labels=c("Low", "Middle", "High")) +
  scale_y_continuous(breaks=c(2, 5, 8),
                   labels=c("Low", "Middle", "High")) +
  JT.theme
@


\begin{marginfigure}
\includegraphics[width=1\textwidth]{ITSL-2tradeoff}
\caption{Interpretability vs. Flexibility}
\label{fig:2tradeoff}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

\subsection{Supervised and Unsupervised Learning}

This must be one of the most unfortunate choices of word to describe what is going on. My intuitive interpretation of these words, before reading about it, was:

\begin{itemize}
  \item \emph{Supervised}: someone, preferably of the human kind, and preferably of an ethical persusasion, keeps watch, so that the artificial learning process stays under control
  \item \emph{Unsupervised}: somewhere, in the dark for further drama, a computer (or many) is/are working on their own and turn out incomprehensible algorithms that quickly enslave humankind
\end{itemize}

\newthought{The real meaning} is:

\begin{itemize}
  \item \emph{Supervised}: the data set we are working on has input variables $X_{i}$ \textbf{AND} an output variable $Y$. Our model building (prediction- or inference-oriented) is not blind because the output variable acts as a supervisor, giving feedback. Methods such as \emph{linear regression}, \emph{logistic regression}, \emph{Generalized Additive Models (GAMs)}, \emph{boosting}, \emph{Support Vector Machines} are supervised.
  \item \emph{Unsupervised}: the data set contains only input variables $X_{i}$. We are working blind because we do not have the output variable $Y$ that we can use in a feedback system. What we can do is e.g. try to find groups of data points that have some characteristic(s) in common. Methods include \emph{cluster analysis}. In a situation where we have $p$ independent variables $X_{i}$ working with scatterplots and linear regression becomes unworkable when $p$ becomes large, because we would need to look at $p(p-1)/2$ graphs.
  \item \emph{Semi-supervised}: a mixture of the above.
\end{itemize}

\subsection{Regression versus Classification Problems}
Variables are either \emph{quantitative} or \emph{qualitative, categorical}. \emph{Quantitative} variables have a numerical value, \emph{Qualtitative} variables take the name, or level, of a class.

The important variable to watch is the \emph{dependent} variable $Y$. The type of the \emph{predictor} variables $X_{i}$ is less important. \emph{Quantitative} problems are referred to as \emph{regression} problems, \emph{qualitative} problems as \emph{classification} problems. But the distinction is not absolute: \emph{linear regression} is used with quantitative problems, but \emph{logistic regression} is used with binary qualitative problems, and its result can then be used for classification. \emph{Qualitative} predictor variables can be switched to \emph{qualitative} by proper coding.

\section{Assessing Model Accuracy}

Every data set is different, and so there is not one method that can be applied to all. We therefore need a way to compare the results of different methods in order to decide which is best.

\subsection{Measuring the Quality of Fit}

How well does our model predict the observed data? A measure is the \emph{mean squared error} (MSE):

\begin{equation}
  MSE = \frac{1}{n}\sum_{i=1}^{i=n} \left(y_{i} - \hat{f}(x_{i})  \right)^{2}
\end{equation}

Here the distinction between the \emph{training set} and the \emph{test set}: we can get the $MSE$ of the \emph{training set} very low by expanding our model but the real test of its prediction power comes when we apply the model to the \emph{test set}. Moreover: most statistical methods specifically try to minimise the MSE of the training set! So a better measure of qualitiy is the average of the squared errors when we use $\hat{f}$ on the \emph{test set}:

\begin{equation}
 Ave\left(y_{0} - \hat{f}(x_{0})   \right)^{2}
\end{equation}

where $y_{O}$ is the value of the dependent variable when $x=x_{0}$ (the \emph{test set}).

An example: let's start from a univariate function $f(x)$:

\begin{equation}
  f(x) = 3 + 0.08x  +0.004x^{2} - 0.0001x^{3}
\end{equaton}

With this function we generate 100 data points, adding a normally distributed error term $\espsilon ~ N(0,sig). From this data set we sample 75 points for the training set and the rest (25) forms the test set. We use the package \textbf{caret}.

<<>>=
library(caret)
N <- 101 # number of points
data.df <- data.frame(x=seq(0, 100, length.out = N), y=0)
sig <- 1
set.seed(2018)
eps <- rnorm(N, 0, sig)
data.df$y <- 3 + 0.08*data.df$x + 0.0035*data.df$x^2 - 0.00004*data.df$x^3
data.df$p <- data.df$y + eps
test.rows <- sample(3:(nrow(data.df)-3), round(N/3, 0), replace=FALSE)
test.df <- data.df[test.rows, ]
train.df <- data.df[-test.rows, ]
ggplot() +
  geom_point(data=train.df, aes(x=x, y=p), pch=16, color="black") +
  geom_line(data=train.df, aes(x=x, y=y), color="black") +
  geom_smooth(data=train.df, aes(x=x, y=p), method=lm, color="red", se=FALSE) +
  stat_smooth(data=train.df, aes(x=x, y=p), method="loess", span=0.25, color="green", se=FALSE) +
  geom_point(data=test.df, aes(x=x, y=p), pch=21, color="blue") +
  JT.theme
# test x-values
new <- data.frame(x=test.df$x)
JT.check <- function(mod, train.p, test.p, new) {
  MSE.train <- mean((train.p - predict(mod))^2)
  MSE.test <- mean((test.p - predict(mod, newdata=new))^2)
  return(c(MSE.train, MSE.test))
}
# checking the linear model
mod.lm <- lm(p ~ x, data=train.df)
JT.check(mod.lm,train.df$p, test.df$p, new)
# checking the loess model for different span widths
n <- 40
MSE.loess <- data.frame(span=seq(0.15, 0.85, length.out = n), flex=0, train=0, test=0)
MSE.loess$flex <- 10^(0.25/MSE.loess$span)
MSE.loess$log.flex <- log10(MSE.loess$flex)
for (i in (1:n)) {
  mod.loess <- loess(p ~ x, data=train.df, span=MSE.loess$span[i], degree = 2)
  MSE.loess$train[i] <- JT.check(mod.loess, train.df$p, test.df$p, new)[1]
  MSE.loess$test[i] <- JT.check(mod.loess, train.df$p, test.df$p, new)[2]
}
ggplot(data=MSE.loess, aes(x=flex)) +
  geom_line(aes(y=train), color="grey") +
  geom_line(aes(y=test), color="red") +
  scale_x_continuous(trans='log10') +
  labs(title="MSE as function of flexibility", x="Flexibitlity", y="Mean Squared Error") +
  JT.theme
@


\newpage
\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}